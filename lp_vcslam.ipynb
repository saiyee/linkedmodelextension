{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from config import VCSLAM\n",
    "from util.parse import generate_dictionaries, generate_id_dict, encode_triples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataset = \"VC-SLAM\"\n",
    "\n",
    "vcslam = VCSLAM()\n",
    "with open(vcslam.PARSED_MODELS_PATH, 'r') as file:\n",
    "    vcslam_models = json.load(file)\n",
    "\n",
    "models = vcslam_models\n",
    "template = vcslam_models\n",
    "triples = []\n",
    "for model in models[:]:\n",
    "    for triple in model:\n",
    "        triples.append(tuple(triple))\n",
    "\n",
    "classes, predicates = generate_dictionaries(triples)\n",
    "classes_mapping = generate_id_dict(classes)\n",
    "predicates_mapping = generate_id_dict(predicates)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "2560"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# triples = modrel.reduce_relations(triples, targets)\n",
    "\n",
    "#test_triples = random.sample(triples, floor(len(triples) / 10))\n",
    "# test_triples = [triple for triple in test_triples if triple[0] == 'http://schema.org/Offer' and triple[2] == str(XSD.string)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Statistics Recommender Baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR VC-SLAM MRR 0.98046875, Hits@1 0.9609375, Hits@3 1.0\n"
     ]
    }
   ],
   "source": [
    "from modelextension.statistics_recommender import StatisticsRecommender as SR\n",
    "from util.metrics import calc_hits_mrr\n",
    "from util.utilities import prepare_data\n",
    "\n",
    "_, _, X_test, y_test = prepare_data(template, c_map=classes_mapping,\n",
    "                    p_map=predicates_mapping, shuffle=False, multiply=1, generate_test=True)\n",
    "\n",
    "encoded_triples = encode_triples(triples, classes_mapping, predicates_mapping)\n",
    "sr = SR(triples=encoded_triples)\n",
    "\n",
    "pred = sr.predict_links(X_test)\n",
    "hits_mrr = calc_hits_mrr( pred, y_test)\n",
    "print(f'SR {dataset} MRR {hits_mrr[\"mrr\"]}, Hits@1 {hits_mrr[\"hits@1\"]}, Hits@3 {hits_mrr[\"hits@3\"]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for i in range(len(classes_mapping.items())):\n",
    "    for j in range(len(classes_mapping.items())):\n",
    "        combinations.append((i,j))\n",
    "\n",
    "# comb_pred = sr.predict_links(combinations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# choices = [np.count_nonzero(prediction) for prediction in comb_pred]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# counts = [count for count in choices if count > 0]\n",
    "# counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train RFC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "%%capture\n",
    "from util.metrics import calc_hits_mrr\n",
    "from util.utilities import prepare_data\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "X, y, X_test, y_test = prepare_data(models = template, multiply=3, c_map=classes_mapping,\n",
    "                    p_map=predicates_mapping, generate_test=True)\n",
    "rfc = RandomForestClassifier(n_estimators=10, max_depth=20)\n",
    "rfc_model = rfc.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC VC-SLAM 10 20 | 0.961 0.927 0.993\n"
     ]
    }
   ],
   "source": [
    "mrrs = []\n",
    "hits_1 = []\n",
    "hits_3 = []\n",
    "pred = rfc_model.predict_proba(X_test)\n",
    "hits_mrr = calc_hits_mrr(pred, y_test)\n",
    "hits_1.append(hits_mrr['hits@1'])\n",
    "hits_3.append(hits_mrr['hits@3'])\n",
    "mrrs.append(hits_mrr['mrr'])\n",
    "\n",
    "print(f'RFC {dataset} 10 20 | {mean(mrrs):.3f} {mean(hits_1):.3f} {mean(hits_3):.3f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train RGCN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training triples size 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\repo\\semantic_recommender\\linkprediction\\utils.py:127: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  norm = 1.0 / in_deg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 MRR 0.05 hits@1 0.01 hits@3 0.06*** | epoch 200 MRR 0.15 hits@1 0.09 hits@3 0.18*** | epoch 300 MRR 0.20 hits@1 0.12 hits@3 0.23*** | epoch 400 MRR 0.25 hits@1 0.18 hits@3 0.27*** | epoch 500 MRR 0.28 hits@1 0.22 hits@3 0.30*** | epoch 600 MRR 0.31 hits@1 0.24 hits@3 0.33*** | epoch 700 MRR 0.34 hits@1 0.26 hits@3 0.36*** | epoch 800 MRR 0.36 hits@1 0.29 hits@3 0.38*** | epoch 900 MRR 0.38 hits@1 0.30 hits@3 0.41*** | epoch 1000 MRR 0.40 hits@1 0.31 hits@3 0.42*** | epoch 1100 MRR 0.42 hits@1 0.33 hits@3 0.46*** | epoch 1200 MRR 0.43 hits@1 0.34 hits@3 0.47*** | epoch 1300 MRR 0.46 hits@1 0.37 hits@3 0.49*** | epoch 1400 MRR 0.47 hits@1 0.38 hits@3 0.51*** | epoch 1500 MRR 0.47 hits@1 0.37 hits@3 0.50*** | epoch 1600 MRR 0.49 hits@1 0.38 hits@3 0.54*** | epoch 1700 MRR 0.51 hits@1 0.40 hits@3 0.57*** | epoch 1800 MRR 0.50 hits@1 0.39 hits@3 0.54 | epoch 1900 MRR 0.53 hits@1 0.41 hits@3 0.58*** | epoch 2000 MRR 0.54 hits@1 0.44 hits@3 0.59*** | epoch 2100 MRR 0.55 hits@1 0.44 hits@3 0.62*** | epoch 2200 MRR 0.58 hits@1 0.48 hits@3 0.64*** | epoch 2300 MRR 0.60 hits@1 0.50 hits@3 0.67*** | epoch 2400 MRR 0.59 hits@1 0.49 hits@3 0.65 | epoch 2500 MRR 0.61 hits@1 0.49 hits@3 0.68*** | epoch 2600 MRR 0.61 hits@1 0.49 hits@3 0.68 | epoch 2700 MRR 0.63 hits@1 0.53 hits@3 0.68*** | epoch 2800 MRR 0.64 hits@1 0.54 hits@3 0.70*** | epoch 2900 MRR 0.65 hits@1 0.55 hits@3 0.72*** | epoch 3000 MRR 0.63 hits@1 0.53 hits@3 0.70 | epoch 3100 MRR 0.63 hits@1 0.52 hits@3 0.70 | epoch 3200 MRR 0.64 hits@1 0.51 hits@3 0.72 | epoch 3300 MRR 0.65 hits@1 0.54 hits@3 0.73 | epoch 3400 MRR 0.67 hits@1 0.56 hits@3 0.73*** | epoch 3500 MRR 0.66 hits@1 0.57 hits@3 0.72 | epoch 3600 MRR 0.68 hits@1 0.57 hits@3 0.75*** | epoch 3700 MRR 0.69 hits@1 0.59 hits@3 0.76*** | epoch 3800 MRR 0.68 hits@1 0.57 hits@3 0.75 | epoch 3900 MRR 0.67 hits@1 0.56 hits@3 0.75 | epoch 4000 MRR 0.68 hits@1 0.56 hits@3 0.77 | epoch 4100 MRR 0.67 hits@1 0.56 hits@3 0.77 | epoch 4200 MRR 0.70 hits@1 0.59 hits@3 0.77*** | epoch 4300 MRR 0.67 hits@1 0.54 hits@3 0.76 | epoch 4400 MRR 0.70 hits@1 0.59 hits@3 0.79*** | epoch 4500 MRR 0.70 hits@1 0.60 hits@3 0.76 | epoch 4600 MRR 0.69 hits@1 0.58 hits@3 0.76 | epoch 4700 MRR 0.67 hits@1 0.54 hits@3 0.75 | epoch 4800 MRR 0.68 hits@1 0.57 hits@3 0.78 | epoch 4900 MRR 0.69 hits@1 0.59 hits@3 0.77 | epoch 5000 MRR 0.70 hits@1 0.59 hits@3 0.78 | epoch 5100 MRR 0.71 hits@1 0.60 hits@3 0.78*** | epoch 5200 MRR 0.72 hits@1 0.62 hits@3 0.77*** | epoch 5300 MRR 0.71 hits@1 0.61 hits@3 0.79 | epoch 5400 MRR 0.71 hits@1 0.61 hits@3 0.79 | epoch 5500 MRR 0.70 hits@1 0.59 hits@3 0.77 | epoch 5600 MRR 0.70 hits@1 0.58 hits@3 0.78 | epoch 5700 MRR 0.73 hits@1 0.63 hits@3 0.79*** | epoch 5800 MRR 0.71 hits@1 0.60 hits@3 0.79 | epoch 5900 MRR 0.71 hits@1 0.59 hits@3 0.80 | epoch 6000 MRR 0.69 hits@1 0.57 hits@3 0.78 | epoch 6100 MRR 0.68 hits@1 0.57 hits@3 0.76 | epoch 6200 MRR 0.71 hits@1 0.60 hits@3 0.78 | epoch 6300 MRR 0.71 hits@1 0.59 hits@3 0.80 | epoch 6400 MRR 0.72 hits@1 0.61 hits@3 0.80 | epoch 6500 MRR 0.74 hits@1 0.63 hits@3 0.80*** | epoch 6600 MRR 0.73 hits@1 0.62 hits@3 0.83 | epoch 6700 MRR 0.75 hits@1 0.65 hits@3 0.81*** | epoch 6800 MRR 0.74 hits@1 0.63 hits@3 0.81 | epoch 6900 MRR 0.75 hits@1 0.64 hits@3 0.82 | epoch 7000 MRR 0.74 hits@1 0.64 hits@3 0.82 | epoch 7100 MRR 0.73 hits@1 0.63 hits@3 0.80 | epoch 7200 MRR 0.74 hits@1 0.62 hits@3 0.81 | epoch 7300 MRR 0.74 hits@1 0.63 hits@3 0.82 | epoch 7400 MRR 0.74 hits@1 0.62 hits@3 0.82 | epoch 7500 MRR 0.75 hits@1 0.64 hits@3 0.83*** | epoch 7600 MRR 0.75 hits@1 0.65 hits@3 0.81*** | epoch 7700 MRR 0.76 hits@1 0.65 hits@3 0.84*** | epoch 7800 MRR 0.74 hits@1 0.63 hits@3 0.82 | epoch 7900 MRR 0.74 hits@1 0.64 hits@3 0.82 | epoch 8000 MRR 0.75 hits@1 0.63 hits@3 0.83 | "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from linkprediction import utils\n",
    "from linkprediction.rgcn import LinkPredict, node_norm_to_edge_norm\n",
    "\n",
    "train_triples = triples\n",
    "test_triples = random.sample(triples, int(len(triples)/10))\n",
    "print(\"training triples size\", len(train_triples))\n",
    "\n",
    "train_data = encode_triples(train_triples, classes_mapping, predicates_mapping)\n",
    "valid_data = encode_triples(test_triples, classes_mapping, predicates_mapping)\n",
    "\n",
    "# load graph data\n",
    "num_nodes = len(classes)\n",
    "num_rels = len(predicates)\n",
    "\n",
    "# create model\n",
    "rgcn_model = LinkPredict(in_dim=num_nodes,\n",
    "                         h_dim=100,\n",
    "                         num_rels=num_rels,\n",
    "                         num_bases=10,\n",
    "                         num_hidden_layers=2,\n",
    "                         dropout=0.1,\n",
    "                         use_cuda=False,\n",
    "                         reg_param=0.01)\n",
    "\n",
    "# validation and testing triplets\n",
    "valid_data = torch.LongTensor(valid_data)\n",
    "test_data = torch.LongTensor(valid_data)\n",
    "\n",
    "# build test graph\n",
    "test_graph, test_rel, test_norm = utils.build_test_graph(\n",
    "    num_nodes, num_rels, train_data)\n",
    "test_deg = test_graph.in_degrees(\n",
    "    range(test_graph.number_of_nodes())).float().view(-1, 1)\n",
    "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
    "test_rel = torch.from_numpy(test_rel)\n",
    "test_norm = node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))\n",
    "\n",
    "# build adj list and calculate degrees for sampling\n",
    "adj_list, degrees = utils.get_adj_and_degrees(num_nodes, train_data)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(rgcn_model.parameters(), lr=0.001)\n",
    "\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "\n",
    "# training loop\n",
    "# print(\"start training...\")\n",
    "\n",
    "epoch = 0\n",
    "best_mrr = 0\n",
    "best_hits3 = 0\n",
    "checkpoint = None\n",
    "while True:\n",
    "    rgcn_model.train()\n",
    "    epoch += 1\n",
    "\n",
    "    # perform edge neighborhood sampling to generate training graph and data\n",
    "    g, node_id, edge_type, node_norm, data, labels = \\\n",
    "        utils.generate_sampled_graph_and_labels(\n",
    "            train_data, 20, 0.3,\n",
    "            num_rels, adj_list, degrees, 5,\n",
    "            \"neighbor\")\n",
    "    # print(\"Done edge sampling\")\n",
    "\n",
    "    # set node/edge feature\n",
    "    node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "    edge_type = torch.from_numpy(edge_type)\n",
    "    edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
    "\n",
    "    embed = rgcn_model(g, node_id, edge_type, edge_norm)\n",
    "    loss = rgcn_model.get_loss(g, embed, data, labels)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rgcn_model.parameters(), 1.0)  # clip gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # validation\n",
    "    if epoch % 100 == 0:\n",
    "        rgcn_model.eval()\n",
    "        # print(\"start eval\")\n",
    "        embed = rgcn_model(test_graph, test_node_id, test_rel, test_norm)\n",
    "        rgcn_results = utils.calc_mrr(embed, rgcn_model.w_relation, torch.LongTensor(train_data),\n",
    "                                      valid_data, test_data, hits=[1, 3], eval_bz=100,\n",
    "                                      eval_p=\"filtered\")\n",
    "        mrr = rgcn_results['mrr']\n",
    "        hits3 = rgcn_results['hits@3']\n",
    "        hits1 = rgcn_results['hits@1']\n",
    "        print(f\"epoch {epoch} MRR {mrr:.2f} hits@1 {hits1:.2f} hits@3 {hits3:.2f}\", end=\"\")\n",
    "        if best_hits3 < hits3:\n",
    "            best_hits3 = hits3\n",
    "        if best_mrr <= mrr:\n",
    "            best_mrr = mrr\n",
    "            checkpoint = {'state_dict': rgcn_model.state_dict(), 'epoch': epoch}\n",
    "            print(f\"*** | \", end=\"\")\n",
    "\n",
    "        else:\n",
    "            print(f\" | \", end=\"\")\n",
    "        # if hits3 == 1:\n",
    "        #    break\n",
    "\n",
    "        if epoch >= 8000:\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best epoch: 7700\n",
      "RGCN VC-SLAM MRR 0.7463410496711731 hits@1 0.62890625 hits@3 0.833984375\n"
     ]
    }
   ],
   "source": [
    "# use best model checkpoint\n",
    "print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n",
    "rgcn_model.eval()\n",
    "rgcn_model.load_state_dict(checkpoint['state_dict'])\n",
    "rgcn_embed = rgcn_model(test_graph, test_node_id, test_rel, test_norm)\n",
    "rgcn_results = utils.calc_mrr(rgcn_embed, rgcn_model.w_relation, torch.LongTensor(train_data), valid_data,\n",
    "                              test_data, hits=[1, 3], eval_bz=100, eval_p=\"filtered\")\n",
    "mrr = rgcn_results['mrr']\n",
    "hits3 = rgcn_results['hits@3']\n",
    "hits1 = rgcn_results['hits@1']\n",
    "print(f\"RGCN {dataset} MRR {mrr} hits@1 {hits1} hits@3 {hits3}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test ME recommendation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "with open(\"modelextensions_single.json\",\"r\") as file:\n",
    "    setups = json.load(fp=file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "with open(\"modelextensions_filtered.json\",\"r\") as file:\n",
    "    setups = json.load(fp=file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "128"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit = len(setups)\n",
    "limit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ME + SR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR ('oracle', 0) \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      " {'mrr': 0.9830729166666666, 'hits@1': 0.96875, 'hits@3': 1.0}\n",
      "SR ('neighbors', 0) \n",
      " [1, 9, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 8, 4, 1, 3, 1, 3, 1, 1, 6, 7, 2, 1, 2, 2, 6, 5, 6, 1, 7, 2, 11, 5, 1, 2, 12, 2, 2, 1, 8, 54, 15, 3, 1, 4, 1, 1, 6, 1, 2, 1, 1, 1, 5, 1, 11, 24, 1, 1, 2, 8, 4, 1, 2, 1, 1, 1, 3, 2, 6, 1, 1, 3, 2, 1, 1, 8, 4, 1, 3, 2, 1, 4, 1, 3, 2, 4, 2, 1, 2, 3, 2, 2, 3, 6, 4, 1, 8, 9, 1, 2, 13, 1, 1, 5, 1, 1, 10, 1, 1, 1, 2, 1, 6, 8, 3, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2] \n",
      " {'mrr': 0.5817202834780959, 'hits@1': 0.390625, 'hits@3': 0.7109375}\n",
      "SR ('similar', 30) \n",
      " [1, 3, 1, 1, 1, 100, 1, 1, 2, 1, 2, 1, 1, 100, 1, 1, 1, 1, 100, 1, 100, 100, 100, 2, 1, 1, 2, 1, 1, 100, 100, 100, 100, 1, 100, 2, 1, 100, 3, 1, 100, 1, 100, 100, 1, 1, 100, 4, 1, 100, 1, 1, 1, 1, 1, 3, 1, 1, 100, 1, 100, 100, 100, 100, 2, 100, 1, 1, 1, 1, 1, 100, 1, 1, 3, 100, 1, 1, 100, 1, 1, 1, 100, 1, 100, 1, 1, 1, 100, 1, 100, 2, 100, 1, 1, 100, 100, 100, 1, 1, 100, 1, 1, 100, 1, 1, 100, 1, 1, 2, 1, 1, 1, 1, 1, 100, 100, 1, 100, 1, 1, 2, 1, 1, 1, 2, 1, 100] \n",
      " {'mrr': 0.6171354166666666, 'hits@1': 0.5625, 'hits@3': 0.671875}\n"
     ]
    }
   ],
   "source": [
    "from util.parse import find_relation\n",
    "from util.metrics import rank\n",
    "\n",
    "modes = [(\"oracle\",0),(\"neighbors\",0),(\"similar\",30)\n",
    "    #, (\"all\", 0)\n",
    "]\n",
    "\n",
    "for mode in modes:\n",
    "    key = mode[0]\n",
    "    me_weight =mode[1] / 100\n",
    "    w_me = me_weight\n",
    "    w_lp = 1-w_me\n",
    "    sr_ranks = []\n",
    "    for setup in setups[:limit]:\n",
    "        anchor = setup['anchor']\n",
    "        target = setup['target']\n",
    "        tuples = [[classes_mapping[anchor], classes_mapping[obj[0]]] for obj in setup[key]]\n",
    "        #print(tuples)\n",
    "        pred = sr.predict_links(tuples)\n",
    "        #print(pred.shape)\n",
    "        predictions = np.argsort(pred)[:, ::-1]\n",
    "        predictions = predictions[:,:10]\n",
    "        #print(valid, predicates_mapping[valid[0][1]], predictions)\n",
    "        probs = [[pred[index][value[i]] for i in range(10)] for  index, value in enumerate(predictions) ]\n",
    "        #print(probs)\n",
    "        sr_result = []\n",
    "        for i in range(len(probs)):\n",
    "            score_me = setup[key][i][1]\n",
    "            obj = setup[key][i][0]\n",
    "            # print(obj, score_me)\n",
    "            for j in range(predictions.shape[1]):\n",
    "                score_lp = probs[i][j]\n",
    "                if score_lp >= 0:\n",
    "                    pred = find_relation(predicates_mapping=predicates_mapping, i=predictions[i][j])\n",
    "                    sr_result.append(((anchor, pred, obj), w_me * score_me + w_lp * score_lp))\n",
    "\n",
    "        #print(sr_result)\n",
    "        sr_result = sorted(sr_result, key=lambda x:x[1], reverse=True)\n",
    "        #print(result[:5])\n",
    "\n",
    "        r = rank(sr_result, target)\n",
    "        #print(r)\n",
    "        sr_ranks.append(r)\n",
    "\n",
    "    # print(len(sr_ranks))\n",
    "    sr_stats = {}\n",
    "    sr_stats['mrr'] = np.mean([1 / rank for rank in sr_ranks])\n",
    "    for value in [1,3]:\n",
    "        hits_at_value = [1 if pos <= value else 0 for pos in sr_ranks]\n",
    "        sr_stats['hits@' + str(value)] = np.sum(hits_at_value) / len(hits_at_value)\n",
    "\n",
    "    print(\"SR\", mode,\"\\n\", sr_ranks, \"\\n\", sr_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ME + RFC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC ('oracle', 0) \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      " {'mrr': 0.9869791666666666, 'hits@1': 0.9765625, 'hits@3': 1.0}\n",
      "RFC ('neighbors', 0) \n",
      " [3, 8, 2, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 25, 6, 1, 3, 1, 1, 1, 2, 4, 6, 5, 1, 3, 2, 3, 4, 3, 1, 2, 1, 6, 3, 1, 1, 6, 1, 1, 4, 3, 59, 7, 5, 1, 8, 5, 1, 5, 1, 6, 1, 1, 1, 5, 4, 5, 22, 2, 7, 3, 6, 3, 1, 2, 2, 3, 1, 1, 2, 6, 3, 1, 3, 1, 1, 1, 10, 3, 1, 4, 2, 1, 5, 1, 3, 5, 5, 1, 1, 2, 10, 1, 1, 18, 13, 7, 1, 6, 8, 1, 3, 11, 1, 1, 4, 1, 1, 6, 1, 1, 1, 1, 1, 8, 7, 2, 6, 3, 1, 2, 1, 3, 4, 2, 2, 1] \n",
      " {'mrr': 0.5717230728605198, 'hits@1': 0.4140625, 'hits@3': 0.6640625}\n",
      "RFC ('similar', 30) \n",
      " [1, 3, 1, 1, 1, 100, 1, 1, 2, 1, 1, 1, 1, 100, 1, 1, 1, 1, 100, 1, 100, 100, 100, 3, 1, 2, 4, 1, 1, 100, 100, 100, 100, 2, 100, 1, 1, 100, 2, 1, 100, 1, 100, 100, 3, 1, 100, 2, 1, 100, 1, 2, 1, 1, 1, 3, 1, 1, 100, 6, 100, 100, 100, 100, 2, 100, 2, 2, 1, 1, 1, 100, 1, 1, 3, 100, 1, 1, 100, 1, 1, 2, 100, 1, 100, 1, 1, 3, 100, 1, 100, 2, 100, 1, 1, 100, 100, 100, 1, 1, 100, 1, 2, 100, 1, 1, 100, 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 1, 100, 2, 1, 2, 2, 3, 1, 2, 2, 100] \n",
      " {'mrr': 0.56765625, 'hits@1': 0.4765625, 'hits@3': 0.6640625}\n"
     ]
    }
   ],
   "source": [
    "from util.parse import find_relation\n",
    "from util.metrics import rank\n",
    "\n",
    "w_me = 0.3\n",
    "w_lp = 1-w_me\n",
    "for mode in modes:\n",
    "    rfc_ranks = []\n",
    "    key = mode[0]\n",
    "    me_weight =mode[1] / 100\n",
    "    w_me = me_weight\n",
    "    for setup in setups[:limit]:\n",
    "        anchor = setup['anchor']\n",
    "        target = setup['target']\n",
    "        tuples = [[classes_mapping[anchor], classes_mapping[obj[0]]] for obj in setup[key]]\n",
    "        #print(tuples)\n",
    "        pred = rfc_model.predict_proba(tuples)\n",
    "        #print(pred, pred.shape)\n",
    "        predictions = np.argsort(pred)[:, ::-1]\n",
    "        predictions = predictions[:,:10]\n",
    "        #print(valid, predicates_mapping[valid[0][1]], predictions)\n",
    "        probs = [[pred[index][value[i]] for i in range(10)] for  index, value in enumerate(predictions) ]\n",
    "        #print(probs)\n",
    "        rfc_result = []\n",
    "        for i in range(len(probs)):\n",
    "            score_me = setup[key][i][1]\n",
    "            obj = setup[key][i][0]\n",
    "            # print(obj, score_me)\n",
    "\n",
    "            for j in range(predictions.shape[1]):\n",
    "                score_lp = probs[i][j]\n",
    "                if score_lp >= 0:\n",
    "                    pred = find_relation(predicates_mapping=predicates_mapping, i=predictions[i][j])\n",
    "                    rfc_result.append(((anchor, pred, obj), w_me * score_me + w_lp * score_lp))\n",
    "\n",
    "        #print(rfc_result)\n",
    "        rfc_result = sorted(rfc_result, key=lambda x:x[1], reverse=True)\n",
    "        # print(len(rfc_result))\n",
    "\n",
    "        r = rank(rfc_result, target)\n",
    "        #print(r)\n",
    "        rfc_ranks.append(r)\n",
    "\n",
    "    rfc_stats = {}\n",
    "    rfc_stats['mrr'] = np.mean([1/rank for rank in rfc_ranks])\n",
    "    for value in [1,3]:\n",
    "        hits_at_value = [1 if pos <= value else 0 for pos in rfc_ranks]\n",
    "        rfc_stats['hits@' + str(value)] = np.sum(hits_at_value) / len(hits_at_value)\n",
    "\n",
    "    print(\"RFC\", mode,\"\\n\", rfc_ranks, \"\\n\", rfc_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ME + RGCN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCN ('oracle', 0) \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 87, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1] \n",
      " {'mrr': 0.9079614844566353, 'hits@1': 0.84375, 'hits@3': 0.96875}\n",
      "RGCN ('neighbors', 0) \n",
      " [1, 2, 3, 2, 1, 2, 1, 9, 3, 2, 1, 2, 1, 6, 4, 2, 7, 1, 2, 1, 1, 7, 31, 1, 1, 3, 3, 11, 2, 17, 4, 12, 5, 6, 6, 2, 1, 7, 2, 1, 6, 7, 3177, 284, 1, 1, 13, 5, 1, 10, 1, 3, 1, 2, 2, 4, 1, 18, 5, 1, 4, 6, 7, 14, 2, 5, 24, 1, 1, 5, 4, 105, 1, 1, 1, 4, 1, 1, 41, 2, 1, 2, 2, 1, 4, 2, 57, 2, 4, 2, 1, 2, 24, 1, 1, 3, 2, 8, 3, 1, 13, 2, 3, 6, 1, 1, 2, 1, 2, 17, 1, 1, 1, 3, 2, 20, 7, 1, 2, 2, 1, 2, 12, 2, 8, 1, 1, 1] \n",
      " {'mrr': 0.5210561895744013, 'hits@1': 0.3359375, 'hits@3': 0.6328125}\n",
      "RGCN ('similar', 30) \n",
      " [1, 1, 1, 1, 1, 100, 1, 1, 2, 1, 1, 1, 1, 100, 6, 1, 1, 1, 100, 1, 100, 100, 100, 1, 1, 1, 3, 6, 2, 100, 100, 100, 100, 2, 100, 2, 3, 100, 1, 1, 100, 19, 100, 100, 1, 1, 100, 6, 1, 100, 1, 1, 1, 2, 2, 5, 1, 3, 100, 1, 100, 100, 100, 100, 2, 100, 14, 1, 1, 2, 3, 100, 1, 1, 2, 100, 1, 1, 100, 1, 1, 2, 100, 1, 100, 2, 11, 1, 100, 2, 100, 3, 100, 1, 1, 100, 100, 100, 2, 1, 100, 2, 2, 100, 1, 1, 100, 1, 5, 3, 1, 1, 1, 6, 3, 100, 100, 2, 100, 1, 1, 3, 5, 2, 1, 1, 2, 100] \n",
      " {'mrr': 0.5043617388642059, 'hits@1': 0.3984375, 'hits@3': 0.6015625}\n"
     ]
    }
   ],
   "source": [
    "from util.parse import find_class\n",
    "\n",
    "for mode in modes:\n",
    "    rgcn_ranks = []\n",
    "    key = mode[0]\n",
    "    me_weight =mode[1] / 100\n",
    "    for setup in setups[:limit]:\n",
    "\n",
    "        anchor = setup['anchor']\n",
    "        target = setup['target']\n",
    "        tuples = [[classes_mapping[anchor], classes_mapping[obj[0]]] for obj in setup[key]]\n",
    "        scores_me = [obj[1] for obj in setup[key]]\n",
    "\n",
    "        candidate_relations = []\n",
    "\n",
    "        for tuple_i, t in enumerate(tuples):\n",
    "            s_id = t[0]\n",
    "            o_id = t[1]\n",
    "            score_me = scores_me[tuple_i]\n",
    "\n",
    "            for pred, i in predicates_mapping.items():\n",
    "                emb_triplet = rgcn_embed[s_id] * rgcn_model.w_relation[i] * rgcn_embed\n",
    "                scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "                scores, indices = torch.sort(scores, descending=True)\n",
    "                p_rank = int((indices == o_id).nonzero())\n",
    "\n",
    "                score_lp = scores[p_rank]\n",
    "                if score_lp >= 0:\n",
    "                    candidate_relations.append(((anchor, pred, find_class(classes_mapping, i=o_id)), w_me * score_me + w_lp * score_lp))\n",
    "\n",
    "        top_list = sorted(candidate_relations, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        rgcn_result = []\n",
    "        # if filter_invalid:\n",
    "        #     for tuple in top_list:\n",
    "        #         if tuple_in_ontology((tuple[0], tuple[1], tuple[2])):\n",
    "        #             result_list.append(tuple)\n",
    "        # else:\n",
    "        rgcn_result = top_list\n",
    "\n",
    "        r = rank(rgcn_result, target)\n",
    "        rgcn_ranks.append(r)\n",
    "\n",
    "    rgcn_stats = {}\n",
    "    rgcn_stats['mrr'] = np.mean([1/rank for rank in rgcn_ranks])\n",
    "    for value in [1,3]:\n",
    "        hits_at_value = [1 if pos <= value else 0 for pos in rgcn_ranks]\n",
    "        rgcn_stats['hits@' + str(value)] = np.sum(hits_at_value) / len(hits_at_value)\n",
    "\n",
    "    print(\"RGCN\", mode,\"\\n\", rgcn_ranks, \"\\n\", rgcn_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}